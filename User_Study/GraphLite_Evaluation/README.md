# Evaluation of GraphLite

We conducted an IRB-approved user study with low-vision screen magnifier users to assess the efficacy of GraphLite and compare it with the status-quo screen magnifier as well as a state-of-the-art usability-enhancing solution.

## 1. Participants

Through emails, word-of-mouth, and assistance from non-profit institutions catering to people with visual disabilities, we managed to recruit 26 low-vision participants (16 female, 10 male) for the study16. To ensure external validity, we did not recruit any participant from the pool of participants who took part in the earlier interview study. The average age of the participants was 51.23 (Median = 9.89, Min = 29, Max = 68). The inclusion criteria were similar to that in our earlier interview study  - the participants had to be: (i) users of screen magnification assistive technology; (ii) familiar with smartphone web browsing, and (iii) familiar with data charts. People with severe visual impairments who could only use a screen reader for interaction were not included in the study. Table presents the participants’ demographic information. All participants stated that they primarily relied on screen magnification to interact with their smartphone browser, only some participants mentioned that in rare cases, they also used a screen reader in addition to screen magnifier when it was too difficult to interact conveniently with just the screen magnifier alone. All participants also mentioned that regularly browsed the web on smartphones and frequently came across bar charts during browsing, especially while surfing news articles. No participant reported other physical difficulties (e.g.,
hearing, motor control) that could affect their ability to perform the study tasks.

## 2. Apparatus

For the study, we built a separate all-in-one Android app supporting all the study conditions. This app was installed on the Samsung Galaxy S23 Ultra smartphone with main display size of 6.8 inches, which was then used in the study. We selected this phone because in the interview study, many participants had mentioned that they preferred to use big-screen smartphones. The smartphone screen was configured to a resolution of 3088 x 1440 pixels and a refresh rate of 120 Hz. The study app offered an array of customizable features aimed at optimizing the presentation of bar charts on smartphone displays. At the start of the session, the bars in the charts were rendered in the default shade of green against a white backdrop, and the default font was Times New Roman with font size of 14px. Each bar had a width of 0.6cm. The app allowed the experimenter to assign charts to conditions and then set the order of the conditions before conducting the study with a participant. The two target task bars in the bar charts were annotated with an easily noticeable black dot similar to those used in prior experiments. Furthermore, in an effort to create an accommodating user interface, we also incorporated the iOS-style panning and zooming functionalities, which could be enabled through the app’s configuration settings. This ensured that participants who were iPhone users could comfortably do the tasks to the best extent possible on our Android device.

## 3. Procedure

The experimenter first obtained the participant’s formal consent before briefly explaining the study’s goals. The experimenter then introduced the TBL and GraphLite interfaces to the participant, and conducted a practice session of about 30 minutes to help the participant get comfortable using the TBL and GraphLite interfaces. After the practice session, the experimenter asked the participant to complete the study tasks in the corresponding predetermined counterbalanced order. The experimenter allotted 10 minutes for the participant to complete each task. After each task (and thereby condition), the experimenter administered the System Usability Scale (SUS) and NASA Task Load Index (NASA-TLX) [33] subjective questionnaires to capture the participant’s perceived usability and interaction workload in that condition. Throughout the study, the experimenter took notes to capture any peculiar user interaction behaviors while doing the tasks. At the end of the study, the experimenter engaged in an open-ended conversation with the participant to collect subjective feedback, including feature requests and improvement ideas. With the participant’s consent, all study activities and data (including screen-capturing) were recorded for later analysis. The participants  were compensated with an Amazon gift card, and all conversations were in English.

### 3.1 Task and Session Distribution

In our study, three sessions were available daily from Monday to Friday over four weeks, with each session lasting between 2 to 2.5 hours. Two experimenters were available per session to accommodate one participant each. Participants had the flexibility to choose any of these sessions to attend. Overall, each participant was required to complete 5 conditions with 6 tasks (4 subtasks + 2 tasks), totaling 300 minutes (5 hours), plus an additional 150 minutes for the remaining study components, resulting in a total of 450 minutes (7.5 hours). With 26 participants in the study, each participant was required to attend multiple sessions to complete the study. Specifically, each participant attended three sessions, totaling 7.5 hours. For example, Participant 1 attended Session 1 on Monday of Week 1, Session 2 on Tuesday of Week 2, and Session 1 on Monday of Week 3. Note that the slowest participant took 4.43 hours to complete just the tasks, which allowed for an additional approximate 3 hours for breaks and other study components. In the first session, participants completed two subtasks involving pairwise comparison. In the second session, they performed two subtasks focused on selective filtering. During the third session, participants engaged in tasks related to line charts, specifically trend prediction and comparison. After each subtask (10 minutes under each condition), participants filled out the SUS and NASA-TLX questionnaires to assess their perceptions of usability and workload. Additionally, the experimenter observed user interaction behaviors and collected feedback during exit interviews.

The time allotment was as follows:

•⁠  ⁠Task 1: Pairwise Comparison 
  - First Subtask : SBC (10 minutes)
  - Second Subtask : MBC (10 minutes)

•⁠  ⁠Task 2: Selective Filtering 
  - First Subtask : SBF (10 minutes)
  - Second Subtask : MBF (10 minutes)
  
•⁠  ⁠Task 3: LTP (10 minutes)

•⁠  ⁠Task 4: LTC (10 minutes)




### Table : EVALUATION USER STUDY PARTICIPANT DEMOGRAPHICS

Note : All information was self-reported

![Alt text](Evaluation_Study.png)
</br>
</br>

## Completion Rate and Usability 
![Alt text](../../Images/Completion.jpeg)
### There are three figures in the picture. The first figure (a) shows the Completion rate (in \%). The second figure (b) presents the boxplots for the TLX scores in all five study conditions. The last figure(c) presents the boxplots for the SUS scores in all five study conditions.

</br>
</br> 

## Accuracy
![Alt text](../../Images/Error.jpeg)
### This figure depicts task accuracy in bar and line chart analyses: comparative and filtration accuracy in bar charts and trend prediction accuracy in line charts, demonstrating participants' analytical proficiency.

